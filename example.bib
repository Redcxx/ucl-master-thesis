@article{example-citation,
    abstract = {{We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.}},
    author = {Anne Author},
    doi = {10.xxxx/example.example.0001},
    journal = {Journal of Classic Examples},
    keywords = {stuff},
    month = jan,
    number = {1},
    pages = {e1001745+},
    pmcid = {PMC3886731},
    pmid = {24415924},
    posted-at = {1970-01-01 00:00:01},
    publisher = {Public Library of Science},
    title = {{Example Journal Paper Title}},
    url = {http://dx.doi.org/10.xxx/example.example.0001},
    volume = {1},
    year = {1970}
}

@article{levinColorizationUsingOptimizationb,
  title = {Colorization Using {{Optimization}}},
  year = {2004},
  author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
  pages = {6},
  abstract = {Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JNDHXNT8\\Levin et al. - Colorization using Optimization.pdf}
}

@misc{ColorizeBlackWhite,
  title = {Colorize {{A Black And White Image Photoshop Tutorial}}},
  journal = {DesignCrowd},
  abstract = {This is a Photoshop tutorial that will guide you through the steps for colorizing a black and white image. Learn how to adjust the levels and colors to cast a vintage photo in a whole new light. Visit blog.designcrowd.com/tag/tutorial for more helpfu},
  howpublished = {https://blog.designcrowd.com/article/889/colorize-a-black-and-white-image},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\Z3FIUK2H\\colorize-a-black-and-white-image-photoshop-tutorial.html}
}


@article{liScribbleBoostAddingClassification2008,
  title = {{{ScribbleBoost}}: {{Adding Classification}} to {{Edge-Aware Interpolation}} of {{Local Image}} and {{Video Adjustments}}},
  shorttitle = {{{ScribbleBoost}}},
  author = {Li, Y. and Adelson, E. and Agarwala, A.},
  year = {2008},
  journal = {Computer Graphics Forum},
  volume = {27},
  number = {4},
  pages = {1255--1264},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2008.01264.x},
  abstract = {One of the most common tasks in image and video editing is the local adjustment of various properties (e.g., saturation or brightness) of regions within an image or video. Edge-aware interpolation of user-drawn scribbles offers a less effort-intensive approach to this problem than traditional region selection and matting. However, the technique suffers a number of limitations, such as reduced performance in the presence of texture contrast, and the inability to handle fragmented appearances. We significantly improve the performance of edge-aware interpolation for this problem by adding a boosting-based classification step that learns to discriminate between the appearance of scribbled pixels. We show that this novel data term in combination with an existing edge-aware optimization technique achieves substantially better results for the local image and video adjustment problem than edge-aware interpolation techniques without classification, or related methods such as matting techniques or graph cut segmentation.},
  langid = {english},
  keywords = {and,Computer,Enhancement,I.4.3,Image,Processing,Vision:},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2008.01264.x},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\W74S28AI\\Li et al. - 2008 - ScribbleBoost Adding Classification to Edge-Aware.pdf;C\:\\Users\\etsun\\Zotero\\storage\\XZ8VHRVE\\j.1467-8659.2008.01264.html}
}




@article{chenManifoldPreservingEdit2012,
  title = {Manifold Preserving Edit Propagation},
  author = {Chen, Xiaowu and Zou, Dongqing and Zhao, Qinping and Tan, Ping},
  year = {2012},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {31},
  number = {6},
  pages = {132:1--132:7},
  issn = {0730-0301},
  doi = {10.1145/2366145.2366151},
  abstract = {We propose a novel edit propagation algorithm for interactive image and video manipulations. Our approach uses the locally linear embedding (LLE) to represent each pixel as a linear combination of its neighbors in a feature space. While previous methods require similar pixels to have similar results, we seek to maintain the manifold structure formed by all pixels in the feature space. Specifically, we require each pixel to be the same linear combination of its neighbors in the result. Compared with previous methods, our proposed algorithm is more robust to color blending in the input data. Furthermore, since every pixel is only related to a few nearest neighbors, our algorithm easily achieves good runtime efficiency. We demonstrate our manifold preserving edit propagation on various applications.},
  keywords = {colorization,edit propagation,manifold preserving,matting,recoloring}
}



@article{endoDeepPropExtractingDeep2016,
  title = {{{DeepProp}}: {{Extracting Deep Features}} from a {{Single Image}} for {{Edit Propagation}}},
  shorttitle = {{{DeepProp}}},
  author = {Endo, Yuki and Iizuka, Satoshi and Kanamori, Yoshihiro and Mitani, Jun},
  year = {2016},
  journal = {Computer Graphics Forum},
  volume = {35},
  number = {2},
  pages = {189--201},
  issn = {1467-8659},
  doi = {10.1111/cgf.12822},
  abstract = {Edit propagation is a technique that can propagate various image edits (e.g., colorization and recoloring) performed via user strokes to the entire image based on similarity of image features. In most previous work, users must manually determine the importance of each image feature (e.g., color, coordinates, and textures) in accordance with their needs and target images. We focus on representation learning that automatically learns feature representations only from user strokes in a single image instead of tuning existing features manually. To this end, this paper proposes an edit propagation method using a deep neural network (DNN). Our DNN, which consists of several layers such as convolutional layers and a feature combiner, extracts stroke-adapted visual features and spatial features, and then adjusts the importance of them. We also develop a learning algorithm for our DNN that does not suffer from the vanishing gradient problem, and hence avoids falling into undesirable locally optimal solutions. We demonstrate that edit propagation with deep features, without manual feature tuning, can achieve better results than previous work.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),I.4.0 Image Processing And Computer Vision: General—},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12822},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H3FQ8UME\\Endo et al. - 2016 - DeepProp Extracting Deep Features from a Single I.pdf}
}



@misc{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2018},
  month = nov,
  number = {arXiv:1611.07004},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{steinsDeepLearningProject2022,
  title = {Deep {{Learning Project}} \textemdash{} {{Anime Illustration Colorization}} \textemdash{} {{Part}} 2},
  author = {Steins},
  year = {2022},
  month = mar,
  journal = {MLearning.ai},
  abstract = {This is an update on my previous article Anime Illustration Colorization with Deep Learning{$\mkern1mu$}\textemdash{$\mkern1mu$}Part 1.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\E65LMNG7\\anime-illustration-colorization-with-deep-learning-part-2-62b1068ef734.html}
}




@misc{PetalicaPaint,
  title = {Petalica {{Paint}}},
  howpublished = {https://petalica-paint.pixiv.dev/index\_en.html},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\PVAAUB66\\index_en.html}
}


@inproceedings{ciUserGuidedDeepAnime2018,
  title = {User-{{Guided Deep Anime Line Art Colorization}} with {{Conditional Adversarial Networks}}},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
  year = {2018},
  month = oct,
  eprint = {1808.03240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1536--1544},
  doi = {10.1145/3240508.3240661},
  abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@article{winnemollerXDoGEXtendedDifferenceofGaussians2012,
  title = {{{XDoG}}: {{An eXtended}} Difference-of-{{Gaussians}} Compendium Including Advanced Image Stylization},
  shorttitle = {{{XDoG}}},
  author = {Winnem{\"o}ller, Holger and Kyprianidis, Jan Eric and Olsen, Sven C.},
  year = {2012},
  month = oct,
  journal = {Computers \& Graphics},
  volume = {36},
  number = {6},
  pages = {740--753},
  issn = {00978493},
  doi = {10.1016/j.cag.2012.03.004},
  abstract = {Recent extensions to the standard difference-of-Gaussians (DoG) edge detection operator have rendered it less susceptible to noise and increased its aesthetic appeal. Despite these advances, the technical subtleties and stylistic potential of the DoG operator are often overlooked. This paper offers a detailed review of the DoG operator and its extensions, highlighting useful relationships to other image processing techniques. It also presents many new results spanning a variety of styles, including pencil-shading, pastel, hatching, and woodcut. Additionally, we demonstrate a range of subtle artistic effects, such as ghosting, speed-lines, negative edges, indication, and abstraction, all of which are obtained using an extended DoG formulation, or slight modifications thereof. In all cases, the visual quality achieved by the extended DoG operator is comparable to or better than those of systems dedicated to a single style.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\5U2K9UJ6\\Winnemöller et al. - 2012 - XDoG An eXtended difference-of-Gaussians compendi.pdf}
}


@misc{fransOutlineColorizationTandem2017,
  title = {Outline {{Colorization}} through {{Tandem Adversarial Networks}}},
  author = {Frans, Kevin},
  year = {2017},
  month = apr,
  number = {arXiv:1704.08834},
  eprint = {1704.08834},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



