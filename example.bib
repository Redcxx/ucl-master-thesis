@article{example-citation,
    abstract = {{We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.}},
    author = {Anne Author},
    doi = {10.xxxx/example.example.0001},
    journal = {Journal of Classic Examples},
    keywords = {stuff},
    month = jan,
    number = {1},
    pages = {e1001745+},
    pmcid = {PMC3886731},
    pmid = {24415924},
    posted-at = {1970-01-01 00:00:01},
    publisher = {Public Library of Science},
    title = {{Example Journal Paper Title}},
    url = {http://dx.doi.org/10.xxx/example.example.0001},
    volume = {1},
    year = {1970}
}

@article{levinColorizationUsingOptimizationb,
  title = {Colorization Using {{Optimization}}},
  year = {2004},
  author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
  pages = {6},
  abstract = {Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JNDHXNT8\\Levin et al. - Colorization using Optimization.pdf}
}

@misc{ColorizeBlackWhite,
  title = {Colorize {{A Black And White Image Photoshop Tutorial}}},
  journal = {DesignCrowd},
  abstract = {This is a Photoshop tutorial that will guide you through the steps for colorizing a black and white image. Learn how to adjust the levels and colors to cast a vintage photo in a whole new light. Visit blog.designcrowd.com/tag/tutorial for more helpfu},
  howpublished = {https://blog.designcrowd.com/article/889/colorize-a-black-and-white-image},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\Z3FIUK2H\\colorize-a-black-and-white-image-photoshop-tutorial.html}
}


@article{liScribbleBoostAddingClassification2008,
  title = {{{ScribbleBoost}}: {{Adding Classification}} to {{Edge-Aware Interpolation}} of {{Local Image}} and {{Video Adjustments}}},
  shorttitle = {{{ScribbleBoost}}},
  author = {Li, Y. and Adelson, E. and Agarwala, A.},
  year = {2008},
  journal = {Computer Graphics Forum},
  volume = {27},
  number = {4},
  pages = {1255--1264},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2008.01264.x},
  abstract = {One of the most common tasks in image and video editing is the local adjustment of various properties (e.g., saturation or brightness) of regions within an image or video. Edge-aware interpolation of user-drawn scribbles offers a less effort-intensive approach to this problem than traditional region selection and matting. However, the technique suffers a number of limitations, such as reduced performance in the presence of texture contrast, and the inability to handle fragmented appearances. We significantly improve the performance of edge-aware interpolation for this problem by adding a boosting-based classification step that learns to discriminate between the appearance of scribbled pixels. We show that this novel data term in combination with an existing edge-aware optimization technique achieves substantially better results for the local image and video adjustment problem than edge-aware interpolation techniques without classification, or related methods such as matting techniques or graph cut segmentation.},
  langid = {english},
  keywords = {and,Computer,Enhancement,I.4.3,Image,Processing,Vision:},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2008.01264.x},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\W74S28AI\\Li et al. - 2008 - ScribbleBoost Adding Classification to Edge-Aware.pdf;C\:\\Users\\etsun\\Zotero\\storage\\XZ8VHRVE\\j.1467-8659.2008.01264.html}
}




@article{chenManifoldPreservingEdit2012,
  title = {Manifold Preserving Edit Propagation},
  author = {Chen, Xiaowu and Zou, Dongqing and Zhao, Qinping and Tan, Ping},
  year = {2012},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {31},
  number = {6},
  pages = {132:1--132:7},
  issn = {0730-0301},
  doi = {10.1145/2366145.2366151},
  abstract = {We propose a novel edit propagation algorithm for interactive image and video manipulations. Our approach uses the locally linear embedding (LLE) to represent each pixel as a linear combination of its neighbors in a feature space. While previous methods require similar pixels to have similar results, we seek to maintain the manifold structure formed by all pixels in the feature space. Specifically, we require each pixel to be the same linear combination of its neighbors in the result. Compared with previous methods, our proposed algorithm is more robust to color blending in the input data. Furthermore, since every pixel is only related to a few nearest neighbors, our algorithm easily achieves good runtime efficiency. We demonstrate our manifold preserving edit propagation on various applications.},
  keywords = {colorization,edit propagation,manifold preserving,matting,recoloring}
}



@article{endoDeepPropExtractingDeep2016,
  title = {{{DeepProp}}: {{Extracting Deep Features}} from a {{Single Image}} for {{Edit Propagation}}},
  shorttitle = {{{DeepProp}}},
  author = {Endo, Yuki and Iizuka, Satoshi and Kanamori, Yoshihiro and Mitani, Jun},
  year = {2016},
  journal = {Computer Graphics Forum},
  volume = {35},
  number = {2},
  pages = {189--201},
  issn = {1467-8659},
  doi = {10.1111/cgf.12822},
  abstract = {Edit propagation is a technique that can propagate various image edits (e.g., colorization and recoloring) performed via user strokes to the entire image based on similarity of image features. In most previous work, users must manually determine the importance of each image feature (e.g., color, coordinates, and textures) in accordance with their needs and target images. We focus on representation learning that automatically learns feature representations only from user strokes in a single image instead of tuning existing features manually. To this end, this paper proposes an edit propagation method using a deep neural network (DNN). Our DNN, which consists of several layers such as convolutional layers and a feature combiner, extracts stroke-adapted visual features and spatial features, and then adjusts the importance of them. We also develop a learning algorithm for our DNN that does not suffer from the vanishing gradient problem, and hence avoids falling into undesirable locally optimal solutions. We demonstrate that edit propagation with deep features, without manual feature tuning, can achieve better results than previous work.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),I.4.0 Image Processing And Computer Vision: Generalâ€”},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12822},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H3FQ8UME\\Endo et al. - 2016 - DeepProp Extracting Deep Features from a Single I.pdf}
}



@misc{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2018},
  month = nov,
  number = {arXiv:1611.07004},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{steinsDeepLearningProject2022,
  title = {Deep {{Learning Project}} \textemdash{} {{Anime Illustration Colorization}} \textemdash{} {{Part}} 2},
  author = {Steins},
  year = {2022},
  month = mar,
  journal = {MLearning.ai},
  abstract = {This is an update on my previous article Anime Illustration Colorization with Deep Learning{$\mkern1mu$}\textemdash{$\mkern1mu$}Part 1.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\E65LMNG7\\anime-illustration-colorization-with-deep-learning-part-2-62b1068ef734.html}
}




@misc{PetalicaPaint,
  title = {Petalica {{Paint}}},
  howpublished = {https://petalica-paint.pixiv.dev/index\_en.html},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\PVAAUB66\\index_en.html}
}


@inproceedings{ciUserGuidedDeepAnime2018,
  title = {User-{{Guided Deep Anime Line Art Colorization}} with {{Conditional Adversarial Networks}}},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
  year = {2018},
  month = oct,
  eprint = {1808.03240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1536--1544},
  doi = {10.1145/3240508.3240661},
  abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@article{winnemollerXDoGEXtendedDifferenceofGaussians2012,
  title = {{{XDoG}}: {{An eXtended}} Difference-of-{{Gaussians}} Compendium Including Advanced Image Stylization},
  shorttitle = {{{XDoG}}},
  author = {Winnem{\"o}ller, Holger and Kyprianidis, Jan Eric and Olsen, Sven C.},
  year = {2012},
  month = oct,
  journal = {Computers \& Graphics},
  volume = {36},
  number = {6},
  pages = {740--753},
  issn = {00978493},
  doi = {10.1016/j.cag.2012.03.004},
  abstract = {Recent extensions to the standard difference-of-Gaussians (DoG) edge detection operator have rendered it less susceptible to noise and increased its aesthetic appeal. Despite these advances, the technical subtleties and stylistic potential of the DoG operator are often overlooked. This paper offers a detailed review of the DoG operator and its extensions, highlighting useful relationships to other image processing techniques. It also presents many new results spanning a variety of styles, including pencil-shading, pastel, hatching, and woodcut. Additionally, we demonstrate a range of subtle artistic effects, such as ghosting, speed-lines, negative edges, indication, and abstraction, all of which are obtained using an extended DoG formulation, or slight modifications thereof. In all cases, the visual quality achieved by the extended DoG operator is comparable to or better than those of systems dedicated to a single style.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\5U2K9UJ6\\WinnemÃ¶ller et al. - 2012 - XDoG An eXtended difference-of-Gaussians compendi.pdf}
}


@misc{fransOutlineColorizationTandem2017,
  title = {Outline {{Colorization}} through {{Tandem Adversarial Networks}}},
  author = {Frans, Kevin},
  year = {2017},
  month = apr,
  number = {arXiv:1704.08834},
  eprint = {1704.08834},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@misc{Undefineda,
  title = {{undefined}},
  journal = {Dwango Media Village(ãƒ‰ãƒ¯ãƒ³ã‚´ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ´ã‚£ãƒ¬ãƒƒã‚¸,dmv)},
  abstract = {undefined},
  howpublished = {https://dmv.nico/en/undefined},
  langid = {ja-jp},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\6TCGT6WP\\nico-opendata.html}
}


@article{branwenDanbooru2021LargeScaleCrowdsourced2015,
  title = {Danbooru2021: {{A Large-Scale Crowdsourced}} and {{Tagged Anime Illustration Dataset}}},
  shorttitle = {Danbooru2021},
  author = {Branwen, Gwern},
  year = {2015},
  month = dec,
  abstract = {Danbooru2021 is a large-scale anime image database with 4.9m+ images annotated with 162m+ tags; it can be useful for machine learning purposes such as image recognition and generation.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\FUNQ93W9\\Danbooru2021.html}
}



@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\8A2TWQV3\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}



@misc{shiRealTimeSingleImage2016,
  title = {Real-{{Time Single Image}} and {{Video Super-Resolution Using}} an {{Efficient Sub-Pixel Convolutional Neural Network}}},
  author = {Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05158},
  eprint = {1609.05158},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@misc{xieAggregatedResidualTransformations2017a,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  year = {2017},
  month = apr,
  number = {arXiv:1611.05431},
  eprint = {1611.05431},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\87MTWYS3\\Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf}
}



@misc{nahDeepMultiscaleConvolutional2018,
  title = {Deep {{Multi-scale Convolutional Neural Network}} for {{Dynamic Scene Deblurring}}},
  author = {Nah, Seungjun and Kim, Tae Hyun and Lee, Kyoung Mu},
  year = {2018},
  month = may,
  number = {arXiv:1612.02177},
  eprint = {1612.02177},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@misc{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1784},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H2Z6M4TQ\\Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf}
}



@misc{ledigPhotoRealisticSingleImage2017,
  title = {Photo-{{Realistic Single Image Super-Resolution Using}} a {{Generative Adversarial Network}}},
  author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year = {2017},
  month = may,
  number = {arXiv:1609.04802},
  eprint = {1609.04802},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@inproceedings{saitoIllustration2VecSemanticVector2015,
  title = {{{Illustration2Vec}}: A Semantic Vector Representation of Illustrations},
  shorttitle = {{{Illustration2Vec}}},
  booktitle = {{{SIGGRAPH Asia}} 2015 {{Technical Briefs}}},
  author = {Saito, Masaki and Matsui, Yusuke},
  year = {2015},
  month = nov,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{Kobe Japan}},
  doi = {10.1145/2820903.2820907},
  abstract = {Referring to existing illustrations helps novice drawers to realize their ideas. To find such helpful references from a large image collection, we first build a semantic vector representation of illustrations by training convolutional neural networks. As the proposed vector space correctly reflects the semantic meanings of illustrations, users can efficiently search for references with similar attributes. Besides the search with a single query, a semantic morphing algorithm that searches the intermediate illustrations that gradually connect two queries is proposed. Several experiments were conducted to demonstrate the effectiveness of our methods.},
  isbn = {978-1-4503-3930-8},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\3GMYEV2B\\Saito and Matsui - 2015 - Illustration2Vec a semantic vector representation.pdf}
}


@misc{ImageNet,
  title = {{{ImageNet}}},
  howpublished = {https://www.image-net.org/},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\CN22ZTZL\\www.image-net.org.html}
}


@misc{pangImagetoImageTranslationMethods2021,
  title = {Image-to-{{Image Translation}}: {{Methods}} and {{Applications}}},
  shorttitle = {Image-to-{{Image Translation}}},
  author = {Pang, Yingxue and Lin, Jianxin and Qin, Tao and Chen, Zhibo},
  year = {2021},
  month = jul,
  number = {arXiv:2101.08629},
  eprint = {2101.08629},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@misc{wangDiscriminativeRegionProposal2018,
  title = {Discriminative {{Region Proposal Adversarial Networks}} for {{High-Quality Image-to-Image Translation}}},
  author = {Wang, Chao and Zheng, Haiyong and Yu, Zhibin and Zheng, Ziqiang and Gu, Zhaorui and Zheng, Bing},
  year = {2018},
  month = aug,
  number = {arXiv:1711.09554},
  eprint = {1711.09554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.09554},
  abstract = {Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement "image inpainting" on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JX69GK9Z\\Wang et al. - 2018 - Discriminative Region Proposal Adversarial Network.pdf}
}


@misc{wangHighResolutionImageSynthesis2018,
  title = {High-{{Resolution Image Synthesis}} and {{Semantic Manipulation}} with {{Conditional GANs}}},
  author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  year = {2018},
  month = aug,
  number = {arXiv:1711.11585},
  eprint = {1711.11585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}


@misc{tangMultiChannelAttentionSelection2019,
  title = {Multi-{{Channel Attention Selection GAN}} with {{Cascaded Semantic Guidance}} for {{Cross-View Image Translation}}},
  author = {Tang, Hao and Xu, Dan and Sebe, Nicu and Wang, Yanzhi and Corso, Jason J. and Yan, Yan},
  year = {2019},
  month = apr,
  number = {arXiv:1904.06807},
  eprint = {1904.06807},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06807},
  abstract = {Cross-view image translation is challenging because it involves images with drastically different views and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top datasets show that our model is able to generate significantly better results than the state-of-the-art methods. The source code, data and trained models are available at https://github.com/Ha0Tang/SelectionGAN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia}
}


@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\HMSH3QZG\\Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf}
}



@misc{baoBEiTBERTPreTraining2021,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  shorttitle = {{{BEiT}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu},
  year = {2021},
  month = jun,
  number = {arXiv:2106.08254},
  eprint = {2106.08254},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.08254},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\RSHP8GLK\\Bao et al. - 2021 - BEiT BERT Pre-Training of Image Transformers.pdf}
}




@misc{weiMaskedFeaturePrediction2021,
  title = {Masked {{Feature Prediction}} for {{Self-Supervised Visual Pre-Training}}},
  author = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  year = {2021},
  month = dec,
  number = {arXiv:2112.09133},
  eprint = {2112.09133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09133},
  abstract = {We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7\% with MViT-L on Kinetics-400, 88.3\% on Kinetics-600, 80.4\% on Kinetics-700, 38.8 mAP on AVA, and 75.0\% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}



@misc{newellHowUsefulSelfSupervised2020,
  title = {How {{Useful}} Is {{Self-Supervised Pretraining}} for {{Visual Tasks}}?},
  author = {Newell, Alejandro and Deng, Jia},
  year = {2020},
  month = mar,
  number = {arXiv:2003.14323},
  eprint = {2003.14323},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \textbackslash href\{https://www.github.com/princeton-vl/selfstudy\}\{github.com/princeton-vl/selfstudy\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\BDFV5PGV\\Newell and Deng - 2020 - How Useful is Self-Supervised Pretraining for Visu.pdf}
}


@misc{sahariaPaletteImagetoImageDiffusion2022,
  title = {Palette: {{Image-to-Image Diffusion Models}}},
  shorttitle = {Palette},
  author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2111.05826},
  eprint = {2111.05826},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.05826},
  abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}



@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05233},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}




@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}




@misc{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}



@misc{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13456},
  eprint = {2011.13456},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.13456},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\textbackslash aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}



@article{fiserShipShapeDrawingBeautification2015,
  title = {{{ShipShape}}: {{A Drawing Beautification Assistant}}},
  shorttitle = {{{ShipShape}}},
  author = {Fi{\v s}er, Jakub and Asente, Paul and S{\'y}kora, Daniel},
  year = {2015},
  journal = {Sketch-Based Interfaces and Modeling},
  pages = {9 pages},
  publisher = {{The Eurographics Association}},
  issn = {1812-3503},
  doi = {10.2312/EXP.20151178},
  abstract = {Sketching is one of the simplest ways to visualize ideas. Its key advantage is requiring the user to have neither deep knowledge of a particular drawing software nor any advanced drawing skills. In practice, however, all these skills become necessary to improve the visual fidelity of the resulting drawing. In this paper, we present ShipShape\textemdash a general beautification assistant that allows users to maintain the simplicity and speed of freehand sketching while still taking into account implicit geometric relations to automatically rectify the output image. In contrast to previous approaches ShipShape works with general B\'ezier curves, enables undo/redo operations, is scale independent, and is fully integrated into Adobe Illustrator. We demonstrate various results to demonstrate capabilities of the proposed method.},
  isbn = {9783905674903},
  langid = {english},
  keywords = {Enhancement,Geometric correction H.5.2 [User Interfaces],Graphics Utilities,I.3.3 [Computer Graphics],I.3.4 [Computer Graphics],I.3.6 [Computer Graphics],I.4.3 [Image Processing and Computer Vision],Input devices and strategies,Interaction techniques,Line and curve generation,Methodology and Techniques,Paint systems,Picture/Image Generation},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\AFJZXRGM\\FiÅ¡er et al. - 2015 - ShipShape A Drawing Beautification Assistant.pdf}
}



@inproceedings{baeILoveSketchAsnaturalaspossibleSketching2008,
  title = {{{ILoveSketch}}: As-Natural-as-Possible Sketching System for Creating 3d Curve Models},
  shorttitle = {{{ILoveSketch}}},
  booktitle = {Proceedings of the 21st Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '08},
  author = {Bae, Seok-Hyung and Balakrishnan, Ravin and Singh, Karan},
  year = {2008},
  pages = {151},
  publisher = {{ACM Press}},
  address = {{Monterey, CA, USA}},
  doi = {10.1145/1449715.1449740},
  abstract = {We present ILoveSketch, a 3D curve sketching system that captures some of the affordances of pen and paper for professional designers, allowing them to iterate directly on concept 3D curve models. The system coherently integrates existing techniques of sketch-based interaction with a number of novel and enhanced features. Novel contributions of the system include automatic view rotation to improve curve sketchability, an axis widget for sketch surface selection, and implicitly inferred changes between sketching techniques. We also improve on a number of existing ideas such as a virtual sketchbook, simplified 2D and 3D view navigation, multi-stroke NURBS curve creation, and a cohesive gesture vocabulary. An evaluation by a professional designer shows the potential of our system for deployment within a real design process.},
  isbn = {978-1-59593-975-3},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\4QYJC4XB\\Bae et al. - 2008 - ILoveSketch as-natural-as-possible sketching syst.pdf}
}


@article{liuClosureawareSketchSimplification2015,
  title = {Closure-Aware Sketch Simplification},
  author = {Liu, Xueting and Wong, Tien-Tsin and Heng, Pheng-Ann},
  year = {2015},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {34},
  number = {6},
  pages = {1--10},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2816795.2818067},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\F5MP593W\\Liu et al. - 2015 - Closure-aware sketch simplification.pdf}
}


@article{simo-serraLearningSimplifyFully2016,
  title = {Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup},
  shorttitle = {Learning to Simplify},
  author = {{Simo-Serra}, Edgar and Iizuka, Satoshi and Sasaki, Kazuma and Ishikawa, Hiroshi},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/2897824.2925972},
  abstract = {In this paper, we present a novel technique to simplify sketch drawings based on learning a series of convolution operators. In contrast to existing approaches that require vector images as input, we allow the more general and challenging input of rough raster sketches such as those obtained from scanning pencil sketches. We convert the rough sketch into a simplified version which is then amendable for vectorization. This is all done in a fully automatic way without user intervention. Our model consists of a fully convolutional neural network which, unlike most existing convolutional neural networks, is able to process images of any dimensions and aspect ratio as input, and outputs a simplified sketch which has the same dimensions as the input image. In order to teach our model to simplify, we present a new dataset of pairs of rough and simplified sketch drawings. By leveraging convolution operators in combination with efficient use of our proposed dataset, we are able to train our sketch simplification model. Our approach naturally overcomes the limitations of existing methods,               e.g.               , vector images as input and long computation time; and we show that meaningful simplifications can be obtained for many different test cases. Finally, we validate our results with a user study in which we greatly outperform similar approaches and establish the state of the art in sketch simplification of raster images.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\KSLJSS2J\\Simo-Serra et al. - 2016 - Learning to simplify fully convolutional networks.pdf}
}



@misc{WelcomePythonOrg,
  title = {Welcome to {{Python}}.Org},
  journal = {Python.org},
  abstract = {The official home of the Python Programming Language},
  howpublished = {https://www.python.org/},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\X25B6B9C\\www.python.org.html}
}


@misc{hendrycksUsingPreTrainingCan2019,
  title = {Using {{Pre-Training Can Improve Model Robustness}} and {{Uncertainty}}},
  author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
  year = {2019},
  month = oct,
  number = {arXiv:1901.09960},
  eprint = {1901.09960},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}



@misc{zagoruykoWideResidualNetworks2017,
  title = {Wide {{Residual Networks}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  month = jun,
  number = {arXiv:1605.07146},
  eprint = {1605.07146},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.07146},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\46QL378J\\Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf}
}



@misc{heRethinkingImageNetPretraining2018,
  title = {Rethinking {{ImageNet Pre-training}}},
  author = {He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  year = {2018},
  month = nov,
  number = {arXiv:1811.08883},
  eprint = {1811.08883},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\6G24ZLRF\\He et al. - 2018 - Rethinking ImageNet Pre-training.pdf;C\:\\Users\\etsun\\Zotero\\storage\\KAATTN5R\\1811.html}
}


@misc{el-noubyAreLargescaleDatasets2021,
  title = {Are {{Large-scale Datasets Necessary}} for {{Self-Supervised Pre-training}}?},
  author = {{El-Nouby}, Alaaeldin and Izacard, Gautier and Touvron, Hugo and Laptev, Ivan and Jegou, Herv{\'e} and Grave, Edouard},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10740},
  eprint = {2112.10740},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Pre-training models on large scale datasets, like ImageNet, is a standard practice in computer vision. This paradigm is especially effective for tasks with small training sets, for which high-capacity models tend to overfit. In this work, we consider a self-supervised pre-training scenario that only leverages the target task data. We consider datasets, like Stanford Cars, Sketch or COCO, which are order(s) of magnitude smaller than Imagenet. Our study shows that denoising autoencoders, such as BEiT or a variant that we introduce in this paper, are more robust to the type and size of the pre-training data than popular self-supervised methods trained by comparing image embeddings.We obtain competitive performance compared to ImageNet pre-training on a variety of classification datasets, from different domains. On COCO, when pre-training solely using COCO images, the detection and instance segmentation performance surpasses the supervised ImageNet pre-training in a comparable setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\W2FKAIAY\\El-Nouby et al. - 2021 - Are Large-scale Datasets Necessary for Self-Superv.pdf}
}



@article{awaisCanPretrainedConvolutional2020,
  title = {Can Pre-Trained Convolutional Neural Networks Be Directly Used as a Feature Extractor for Video-Based Neonatal Sleep and Wake Classification?},
  author = {Awais, Muhammad and Long, Xi and Yin, Bin and Chen, Chen and Akbarzadeh, Saeed and Abbasi, Saadullah Farooq and Irfan, Muhammad and Lu, Chunmei and Wang, Xinhua and Wang, Laishuan and Chen, Wei},
  year = {2020},
  month = dec,
  journal = {BMC Research Notes},
  volume = {13},
  number = {1},
  pages = {507},
  issn = {1756-0500},
  doi = {10.1186/s13104-020-05343-4},
  abstract = {Objective:\hspace{0.6em} In this paper, we propose to evaluate the use of pre-trained convolutional neural networks (CNNs) as a features extractor followed by the Principal Component Analysis (PCA) to find the best discriminant features to perform classification using support vector machine (SVM) algorithm for neonatal sleep and wake states using F\- luke\textregistered{} facial video frames. Using pre-trained CNNs as a feature extractor would hugely reduce the effort of collecting new neonatal data for training a neural network which could be computationally expensive. The features are extracted after fully connected layers (FCL's), where we compare several pre-trained CNNs, e.g., VGG16, VGG19, InceptionV3, GoogLeNet, ResNet, and AlexNet. Results:\hspace{0.6em} From around 2-h \-Fluke\textregistered{} video recording of seven neonates, we achieved a modest classification performance with an accuracy, sensitivity, and specificity of 65.3\%, 69.8\%, 61.0\%, respectively with AlexNet using F\- luke\textregistered{} (RGB) video frames. This indicates that using a pre-trained model as a feature extractor could not fully suffice for highly reliable sleep and wake classification in neonates. Therefore, in future work a dedicated neural network trained on neonatal data or a transfer learning approach is required.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\VEKRMDMF\\Awais et al. - 2020 - Can pre-trained convolutional neural networks be d.pdf}
}



@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11929},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}



@inproceedings{hintonStochasticNeighborEmbedding2002,
  title = {Stochastic {{Neighbor Embedding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hinton, Geoffrey E and Roweis, Sam},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  abstract = {We describe a probabilistic approach to the task of placing objects, de- scribed by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribu- tion as well as possible when the same operation is performed on the low-dimensional ``images'' of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional im- ages. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ``bank'', to have versions close to the images of both ``river'' and ``finance'' without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\FS5UCSF7\\Hinton and Roweis - 2002 - Stochastic Neighbor Embedding.pdf}
}




@article{erhanWhyDoesUnsupervised,
  title = {Why {{Does Unsupervised Pre-training Help Deep Learning}}?},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  pages = {36},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\LRQ9WIBT\\Erhan et al. - Why Does Unsupervised Pre-training Help Deep Learn.pdf}
}


@misc{longFullyConvolutionalNetworks2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  month = mar,
  number = {arXiv:1411.4038},
  eprint = {1411.4038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\B9L7WYG9\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;C\:\\Users\\etsun\\Zotero\\storage\\9KCQPGLJ\\1411.html}
}


@misc{goodfellowGenerativeAdversarialNetworks2014a,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\22U9VPA4\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;C\:\\Users\\etsun\\Zotero\\storage\\8YZUMCDJ\\1406.html}
}



@misc{arjovskyPrincipledMethodsTraining2017,
  title = {Towards {{Principled Methods}} for {{Training Generative Adversarial Networks}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on},
  year = {2017},
  month = jan,
  number = {arXiv:1701.04862},
  eprint = {1701.04862},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\8SD2TIDC\\Arjovsky and Bottou - 2017 - Towards Principled Methods for Training Generative.pdf;C\:\\Users\\etsun\\Zotero\\storage\\58IKZ2MG\\1701.html}
}


@misc{ioffeBatchNormalizationAccelerating2015a,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\DAR7TWET\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf}
}


@inproceedings{santurkarHowDoesBatch2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\UZWFPPDH\\Santurkar et al. - 2018 - How Does Batch Normalization Help Optimization.pdf}
}


@inproceedings{glorotDeepSparseRectifier2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  pages = {315--323},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\FJ4UHEF8\\Glorot et al. - 2011 - Deep Sparse Rectifier Neural Networks.pdf}
}


@misc{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = dec,
  number = {arXiv:1701.07875},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}


@misc{FileWassersteinGANCritic,
  title = {File:{{Wasserstein-GAN}} Critic vs Log-Minus {{GAN}} Discriminator.Svg - {{Wikipedia}}},
  shorttitle = {File},
  howpublished = {https://commons.wikimedia.org/wiki/File:Wasserstein-GAN\_critic\_vs\_log-minus\_GAN\_discriminator.svg},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\N84UU6M8\\FileWasserstein-GAN_critic_vs_log-minus_GAN_discriminator.html}
}


@misc{gulrajaniImprovedTrainingWasserstein2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  year = {2017},
  month = dec,
  number = {arXiv:1704.00028},
  eprint = {1704.00028},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.00028},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}


@misc{heDeepResidualLearning2015a,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\4ZVJHKHX\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}



@misc{heIdentityMappingsDeep2016a,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jul,
  number = {arXiv:1603.05027},
  eprint = {1603.05027},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\27VAE82G\\He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf}
}


@misc{zhangRealTimeUserGuidedImage2017,
  title = {Real-{{Time User-Guided Image Colorization}} with {{Learned Deep Priors}}},
  author = {Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S. and Yu, Tianhe and Efros, Alexei A.},
  year = {2017},
  month = may,
  number = {arXiv:1705.02999},
  eprint = {1705.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user "hints" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user "hints" to the desired colorization, showing an application to color histogram transfer. Our code and models are available at https://richzhang.github.io/ideepcolor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\RW286KEM\\Zhang et al. - 2017 - Real-Time User-Guided Image Colorization with Lear.pdf}
}


@article{DiscreteWaveletTransform2022,
  title = {Discrete Wavelet Transform},
  year = {2022},
  month = jul,
  journal = {Wikipedia},
  abstract = {In numerical analysis and functional analysis, a discrete wavelet transform (DWT) is any wavelet transform for which the wavelets are discretely sampled. As with other wavelet transforms, a key advantage it has over Fourier transforms is temporal resolution: it captures both frequency and location information (location in time).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1096307202},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\82FU747A\\Discrete_wavelet_transform.html}
}


@article{yanBenchmarkRoughSketch2020,
  title = {A Benchmark for Rough Sketch Cleanup},
  author = {Yan, Chuan and Vanderhaeghe, David and Gingold, Yotam},
  year = {2020},
  month = dec,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {6},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3414685.3417784},
  abstract = {Our evaluation identifies shortcomings among state-of-the-art cleanup algorithms and discusses open problems for future research.  CCS Concepts: \textbullet{} Computing methodologies Image processing;  Graphics systems and interfaces; Shape analysis; \textbullet{} Applied computing Computer-aided design.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\I5VSSVHK\\Yan et al. - 2020 - A benchmark for rough sketch cleanup.pdf}
}


@misc{simo-serraMasteringSketchingAdversarial2017,
  title = {Mastering {{Sketching}}: {{Adversarial Augmentation}} for {{Structured Prediction}}},
  shorttitle = {Mastering {{Sketching}}},
  author = {{Simo-Serra}, Edgar and Iizuka, Satoshi and Ishikawa, Hiroshi},
  year = {2017},
  month = mar,
  number = {arXiv:1703.08966},
  eprint = {1703.08966},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present an integral framework for training sketch simplification networks that convert challenging rough sketches into clean line drawings. Our approach augments a simplification network with a discriminator network, training both networks jointly so that the discriminator network discerns whether a line drawing is a real training data or the output of the simplification network, which in turn tries to fool it. This approach has two major advantages. First, because the discriminator network learns the structure in line drawings, it encourages the output sketches of the simplification network to be more similar in appearance to the training sketches. Second, we can also train the simplification network with additional unsupervised data, using the discriminator network as a substitute teacher. Thus, by adding only rough sketches without simplified line drawings, or only line drawings without the original rough sketches, we can improve the quality of the sketch simplification. We show how our framework can be used to train models that significantly outperform the state of the art in the sketch simplification task, despite using the same architecture for inference. We additionally present an approach to optimize for a single image, which improves accuracy at the cost of additional computation time. Finally, we show that, using the same framework, it is possible to train the network to perform the inverse problem, i.e., convert simple line sketches into pencil drawings, which is not possible using the standard mean squared error loss. We validate our framework with two user tests, where our approach is preferred to the state of the art in sketch simplification 92.3\% of the time and obtains 1.2 more points on a scale of 1 to 5.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

























