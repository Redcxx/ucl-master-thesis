@article{example-citation,
    abstract = {{We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.}},
    author = {Anne Author},
    doi = {10.xxxx/example.example.0001},
    journal = {Journal of Classic Examples},
    keywords = {stuff},
    month = jan,
    number = {1},
    pages = {e1001745+},
    pmcid = {PMC3886731},
    pmid = {24415924},
    posted-at = {1970-01-01 00:00:01},
    publisher = {Public Library of Science},
    title = {{Example Journal Paper Title}},
    url = {http://dx.doi.org/10.xxx/example.example.0001},
    volume = {1},
    year = {1970}
}

@article{levinColorizationUsingOptimizationb,
  title = {Colorization Using {{Optimization}}},
  year = {2004},
  author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
  pages = {6},
  abstract = {Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JNDHXNT8\\Levin et al. - Colorization using Optimization.pdf}
}

@misc{ColorizeBlackWhite,
  title = {Colorize {{A Black And White Image Photoshop Tutorial}}},
  journal = {DesignCrowd},
  abstract = {This is a Photoshop tutorial that will guide you through the steps for colorizing a black and white image. Learn how to adjust the levels and colors to cast a vintage photo in a whole new light. Visit blog.designcrowd.com/tag/tutorial for more helpfu},
  howpublished = {https://blog.designcrowd.com/article/889/colorize-a-black-and-white-image},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\Z3FIUK2H\\colorize-a-black-and-white-image-photoshop-tutorial.html}
}


@article{liScribbleBoostAddingClassification2008,
  title = {{{ScribbleBoost}}: {{Adding Classification}} to {{Edge-Aware Interpolation}} of {{Local Image}} and {{Video Adjustments}}},
  shorttitle = {{{ScribbleBoost}}},
  author = {Li, Y. and Adelson, E. and Agarwala, A.},
  year = {2008},
  journal = {Computer Graphics Forum},
  volume = {27},
  number = {4},
  pages = {1255--1264},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2008.01264.x},
  abstract = {One of the most common tasks in image and video editing is the local adjustment of various properties (e.g., saturation or brightness) of regions within an image or video. Edge-aware interpolation of user-drawn scribbles offers a less effort-intensive approach to this problem than traditional region selection and matting. However, the technique suffers a number of limitations, such as reduced performance in the presence of texture contrast, and the inability to handle fragmented appearances. We significantly improve the performance of edge-aware interpolation for this problem by adding a boosting-based classification step that learns to discriminate between the appearance of scribbled pixels. We show that this novel data term in combination with an existing edge-aware optimization technique achieves substantially better results for the local image and video adjustment problem than edge-aware interpolation techniques without classification, or related methods such as matting techniques or graph cut segmentation.},
  langid = {english},
  keywords = {and,Computer,Enhancement,I.4.3,Image,Processing,Vision:},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2008.01264.x},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\W74S28AI\\Li et al. - 2008 - ScribbleBoost Adding Classification to Edge-Aware.pdf;C\:\\Users\\etsun\\Zotero\\storage\\XZ8VHRVE\\j.1467-8659.2008.01264.html}
}




@article{chenManifoldPreservingEdit2012,
  title = {Manifold Preserving Edit Propagation},
  author = {Chen, Xiaowu and Zou, Dongqing and Zhao, Qinping and Tan, Ping},
  year = {2012},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {31},
  number = {6},
  pages = {132:1--132:7},
  issn = {0730-0301},
  doi = {10.1145/2366145.2366151},
  abstract = {We propose a novel edit propagation algorithm for interactive image and video manipulations. Our approach uses the locally linear embedding (LLE) to represent each pixel as a linear combination of its neighbors in a feature space. While previous methods require similar pixels to have similar results, we seek to maintain the manifold structure formed by all pixels in the feature space. Specifically, we require each pixel to be the same linear combination of its neighbors in the result. Compared with previous methods, our proposed algorithm is more robust to color blending in the input data. Furthermore, since every pixel is only related to a few nearest neighbors, our algorithm easily achieves good runtime efficiency. We demonstrate our manifold preserving edit propagation on various applications.},
  keywords = {colorization,edit propagation,manifold preserving,matting,recoloring}
}



@article{endoDeepPropExtractingDeep2016,
  title = {{{DeepProp}}: {{Extracting Deep Features}} from a {{Single Image}} for {{Edit Propagation}}},
  shorttitle = {{{DeepProp}}},
  author = {Endo, Yuki and Iizuka, Satoshi and Kanamori, Yoshihiro and Mitani, Jun},
  year = {2016},
  journal = {Computer Graphics Forum},
  volume = {35},
  number = {2},
  pages = {189--201},
  issn = {1467-8659},
  doi = {10.1111/cgf.12822},
  abstract = {Edit propagation is a technique that can propagate various image edits (e.g., colorization and recoloring) performed via user strokes to the entire image based on similarity of image features. In most previous work, users must manually determine the importance of each image feature (e.g., color, coordinates, and textures) in accordance with their needs and target images. We focus on representation learning that automatically learns feature representations only from user strokes in a single image instead of tuning existing features manually. To this end, this paper proposes an edit propagation method using a deep neural network (DNN). Our DNN, which consists of several layers such as convolutional layers and a feature combiner, extracts stroke-adapted visual features and spatial features, and then adjusts the importance of them. We also develop a learning algorithm for our DNN that does not suffer from the vanishing gradient problem, and hence avoids falling into undesirable locally optimal solutions. We demonstrate that edit propagation with deep features, without manual feature tuning, can achieve better results than previous work.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),I.4.0 Image Processing And Computer Vision: General—},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12822},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H3FQ8UME\\Endo et al. - 2016 - DeepProp Extracting Deep Features from a Single I.pdf}
}



@misc{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2018},
  month = nov,
  number = {arXiv:1611.07004},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{steinsDeepLearningProject2022,
  title = {Deep {{Learning Project}} \textemdash{} {{Anime Illustration Colorization}} \textemdash{} {{Part}} 2},
  author = {Steins},
  year = {2022},
  month = mar,
  journal = {MLearning.ai},
  abstract = {This is an update on my previous article Anime Illustration Colorization with Deep Learning{$\mkern1mu$}\textemdash{$\mkern1mu$}Part 1.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\E65LMNG7\\anime-illustration-colorization-with-deep-learning-part-2-62b1068ef734.html}
}




@misc{PetalicaPaint,
  title = {Petalica {{Paint}}},
  howpublished = {https://petalica-paint.pixiv.dev/index\_en.html},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\PVAAUB66\\index_en.html}
}


@inproceedings{ciUserGuidedDeepAnime2018,
  title = {User-{{Guided Deep Anime Line Art Colorization}} with {{Conditional Adversarial Networks}}},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
  year = {2018},
  month = oct,
  eprint = {1808.03240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1536--1544},
  doi = {10.1145/3240508.3240661},
  abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@article{winnemollerXDoGEXtendedDifferenceofGaussians2012,
  title = {{{XDoG}}: {{An eXtended}} Difference-of-{{Gaussians}} Compendium Including Advanced Image Stylization},
  shorttitle = {{{XDoG}}},
  author = {Winnem{\"o}ller, Holger and Kyprianidis, Jan Eric and Olsen, Sven C.},
  year = {2012},
  month = oct,
  journal = {Computers \& Graphics},
  volume = {36},
  number = {6},
  pages = {740--753},
  issn = {00978493},
  doi = {10.1016/j.cag.2012.03.004},
  abstract = {Recent extensions to the standard difference-of-Gaussians (DoG) edge detection operator have rendered it less susceptible to noise and increased its aesthetic appeal. Despite these advances, the technical subtleties and stylistic potential of the DoG operator are often overlooked. This paper offers a detailed review of the DoG operator and its extensions, highlighting useful relationships to other image processing techniques. It also presents many new results spanning a variety of styles, including pencil-shading, pastel, hatching, and woodcut. Additionally, we demonstrate a range of subtle artistic effects, such as ghosting, speed-lines, negative edges, indication, and abstraction, all of which are obtained using an extended DoG formulation, or slight modifications thereof. In all cases, the visual quality achieved by the extended DoG operator is comparable to or better than those of systems dedicated to a single style.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\5U2K9UJ6\\Winnemöller et al. - 2012 - XDoG An eXtended difference-of-Gaussians compendi.pdf}
}


@misc{fransOutlineColorizationTandem2017,
  title = {Outline {{Colorization}} through {{Tandem Adversarial Networks}}},
  author = {Frans, Kevin},
  year = {2017},
  month = apr,
  number = {arXiv:1704.08834},
  eprint = {1704.08834},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@misc{Undefineda,
  title = {{undefined}},
  journal = {Dwango Media Village(ドワンゴメディアヴィレッジ,dmv)},
  abstract = {undefined},
  howpublished = {https://dmv.nico/en/undefined},
  langid = {ja-jp},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\6TCGT6WP\\nico-opendata.html}
}


@article{branwenDanbooru2021LargeScaleCrowdsourced2015,
  title = {Danbooru2021: {{A Large-Scale Crowdsourced}} and {{Tagged Anime Illustration Dataset}}},
  shorttitle = {Danbooru2021},
  author = {Branwen, Gwern},
  year = {2015},
  month = dec,
  abstract = {Danbooru2021 is a large-scale anime image database with 4.9m+ images annotated with 162m+ tags; it can be useful for machine learning purposes such as image recognition and generation.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\FUNQ93W9\\Danbooru2021.html}
}



@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\8A2TWQV3\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}



@misc{shiRealTimeSingleImage2016,
  title = {Real-{{Time Single Image}} and {{Video Super-Resolution Using}} an {{Efficient Sub-Pixel Convolutional Neural Network}}},
  author = {Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05158},
  eprint = {1609.05158},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@misc{xieAggregatedResidualTransformations2017a,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  year = {2017},
  month = apr,
  number = {arXiv:1611.05431},
  eprint = {1611.05431},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\87MTWYS3\\Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf}
}



@misc{nahDeepMultiscaleConvolutional2018,
  title = {Deep {{Multi-scale Convolutional Neural Network}} for {{Dynamic Scene Deblurring}}},
  author = {Nah, Seungjun and Kim, Tae Hyun and Lee, Kyoung Mu},
  year = {2018},
  month = may,
  number = {arXiv:1612.02177},
  eprint = {1612.02177},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@misc{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1784},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H2Z6M4TQ\\Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf}
}



@misc{ledigPhotoRealisticSingleImage2017,
  title = {Photo-{{Realistic Single Image Super-Resolution Using}} a {{Generative Adversarial Network}}},
  author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year = {2017},
  month = may,
  number = {arXiv:1609.04802},
  eprint = {1609.04802},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@inproceedings{saitoIllustration2VecSemanticVector2015,
  title = {{{Illustration2Vec}}: A Semantic Vector Representation of Illustrations},
  shorttitle = {{{Illustration2Vec}}},
  booktitle = {{{SIGGRAPH Asia}} 2015 {{Technical Briefs}}},
  author = {Saito, Masaki and Matsui, Yusuke},
  year = {2015},
  month = nov,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{Kobe Japan}},
  doi = {10.1145/2820903.2820907},
  abstract = {Referring to existing illustrations helps novice drawers to realize their ideas. To find such helpful references from a large image collection, we first build a semantic vector representation of illustrations by training convolutional neural networks. As the proposed vector space correctly reflects the semantic meanings of illustrations, users can efficiently search for references with similar attributes. Besides the search with a single query, a semantic morphing algorithm that searches the intermediate illustrations that gradually connect two queries is proposed. Several experiments were conducted to demonstrate the effectiveness of our methods.},
  isbn = {978-1-4503-3930-8},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\3GMYEV2B\\Saito and Matsui - 2015 - Illustration2Vec a semantic vector representation.pdf}
}


@misc{ImageNet,
  title = {{{ImageNet}}},
  howpublished = {https://www.image-net.org/},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\CN22ZTZL\\www.image-net.org.html}
}




