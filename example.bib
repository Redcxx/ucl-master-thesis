@article{example-citation,
    abstract = {{We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.}},
    author = {Anne Author},
    doi = {10.xxxx/example.example.0001},
    journal = {Journal of Classic Examples},
    keywords = {stuff},
    month = jan,
    number = {1},
    pages = {e1001745+},
    pmcid = {PMC3886731},
    pmid = {24415924},
    posted-at = {1970-01-01 00:00:01},
    publisher = {Public Library of Science},
    title = {{Example Journal Paper Title}},
    url = {http://dx.doi.org/10.xxx/example.example.0001},
    volume = {1},
    year = {1970}
}

@article{levinColorizationUsingOptimizationb,
  title = {Colorization Using {{Optimization}}},
  year = {2004},
  author = {Levin, Anat and Lischinski, Dani and Weiss, Yair},
  pages = {6},
  abstract = {Colorization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colorization requires considerable user intervention and remains a tedious, time-consuming, and expensive task.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JNDHXNT8\\Levin et al. - Colorization using Optimization.pdf}
}

@misc{ColorizeBlackWhite,
  title = {Colorize {{A Black And White Image Photoshop Tutorial}}},
  journal = {DesignCrowd},
  abstract = {This is a Photoshop tutorial that will guide you through the steps for colorizing a black and white image. Learn how to adjust the levels and colors to cast a vintage photo in a whole new light. Visit blog.designcrowd.com/tag/tutorial for more helpfu},
  howpublished = {https://blog.designcrowd.com/article/889/colorize-a-black-and-white-image},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\Z3FIUK2H\\colorize-a-black-and-white-image-photoshop-tutorial.html}
}


@article{liScribbleBoostAddingClassification2008,
  title = {{{ScribbleBoost}}: {{Adding Classification}} to {{Edge-Aware Interpolation}} of {{Local Image}} and {{Video Adjustments}}},
  shorttitle = {{{ScribbleBoost}}},
  author = {Li, Y. and Adelson, E. and Agarwala, A.},
  year = {2008},
  journal = {Computer Graphics Forum},
  volume = {27},
  number = {4},
  pages = {1255--1264},
  issn = {1467-8659},
  doi = {10.1111/j.1467-8659.2008.01264.x},
  abstract = {One of the most common tasks in image and video editing is the local adjustment of various properties (e.g., saturation or brightness) of regions within an image or video. Edge-aware interpolation of user-drawn scribbles offers a less effort-intensive approach to this problem than traditional region selection and matting. However, the technique suffers a number of limitations, such as reduced performance in the presence of texture contrast, and the inability to handle fragmented appearances. We significantly improve the performance of edge-aware interpolation for this problem by adding a boosting-based classification step that learns to discriminate between the appearance of scribbled pixels. We show that this novel data term in combination with an existing edge-aware optimization technique achieves substantially better results for the local image and video adjustment problem than edge-aware interpolation techniques without classification, or related methods such as matting techniques or graph cut segmentation.},
  langid = {english},
  keywords = {and,Computer,Enhancement,I.4.3,Image,Processing,Vision:},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2008.01264.x},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\W74S28AI\\Li et al. - 2008 - ScribbleBoost Adding Classification to Edge-Aware.pdf;C\:\\Users\\etsun\\Zotero\\storage\\XZ8VHRVE\\j.1467-8659.2008.01264.html}
}




@article{chenManifoldPreservingEdit2012,
  title = {Manifold Preserving Edit Propagation},
  author = {Chen, Xiaowu and Zou, Dongqing and Zhao, Qinping and Tan, Ping},
  year = {2012},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {31},
  number = {6},
  pages = {132:1--132:7},
  issn = {0730-0301},
  doi = {10.1145/2366145.2366151},
  abstract = {We propose a novel edit propagation algorithm for interactive image and video manipulations. Our approach uses the locally linear embedding (LLE) to represent each pixel as a linear combination of its neighbors in a feature space. While previous methods require similar pixels to have similar results, we seek to maintain the manifold structure formed by all pixels in the feature space. Specifically, we require each pixel to be the same linear combination of its neighbors in the result. Compared with previous methods, our proposed algorithm is more robust to color blending in the input data. Furthermore, since every pixel is only related to a few nearest neighbors, our algorithm easily achieves good runtime efficiency. We demonstrate our manifold preserving edit propagation on various applications.},
  keywords = {colorization,edit propagation,manifold preserving,matting,recoloring}
}



@article{endoDeepPropExtractingDeep2016,
  title = {{{DeepProp}}: {{Extracting Deep Features}} from a {{Single Image}} for {{Edit Propagation}}},
  shorttitle = {{{DeepProp}}},
  author = {Endo, Yuki and Iizuka, Satoshi and Kanamori, Yoshihiro and Mitani, Jun},
  year = {2016},
  journal = {Computer Graphics Forum},
  volume = {35},
  number = {2},
  pages = {189--201},
  issn = {1467-8659},
  doi = {10.1111/cgf.12822},
  abstract = {Edit propagation is a technique that can propagate various image edits (e.g., colorization and recoloring) performed via user strokes to the entire image based on similarity of image features. In most previous work, users must manually determine the importance of each image feature (e.g., color, coordinates, and textures) in accordance with their needs and target images. We focus on representation learning that automatically learns feature representations only from user strokes in a single image instead of tuning existing features manually. To this end, this paper proposes an edit propagation method using a deep neural network (DNN). Our DNN, which consists of several layers such as convolutional layers and a feature combiner, extracts stroke-adapted visual features and spatial features, and then adjusts the importance of them. We also develop a learning algorithm for our DNN that does not suffer from the vanishing gradient problem, and hence avoids falling into undesirable locally optimal solutions. We demonstrate that edit propagation with deep features, without manual feature tuning, can achieve better results than previous work.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),I.4.0 Image Processing And Computer Vision: Generalâ€”},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12822},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H3FQ8UME\\Endo et al. - 2016 - DeepProp Extracting Deep Features from a Single I.pdf}
}



@misc{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2018},
  month = nov,
  number = {arXiv:1611.07004},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07004},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@misc{steinsDeepLearningProject2022,
  title = {Deep {{Learning Project}} \textemdash{} {{Anime Illustration Colorization}} \textemdash{} {{Part}} 2},
  author = {Steins},
  year = {2022},
  month = mar,
  journal = {MLearning.ai},
  abstract = {This is an update on my previous article Anime Illustration Colorization with Deep Learning{$\mkern1mu$}\textemdash{$\mkern1mu$}Part 1.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\E65LMNG7\\anime-illustration-colorization-with-deep-learning-part-2-62b1068ef734.html}
}




@misc{PetalicaPaint,
  title = {Petalica {{Paint}}},
  howpublished = {https://petalica-paint.pixiv.dev/index\_en.html},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\PVAAUB66\\index_en.html}
}


@inproceedings{ciUserGuidedDeepAnime2018,
  title = {User-{{Guided Deep Anime Line Art Colorization}} with {{Conditional Adversarial Networks}}},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
  year = {2018},
  month = oct,
  eprint = {1808.03240},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1536--1544},
  doi = {10.1145/3240508.3240661},
  abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@article{winnemollerXDoGEXtendedDifferenceofGaussians2012,
  title = {{{XDoG}}: {{An eXtended}} Difference-of-{{Gaussians}} Compendium Including Advanced Image Stylization},
  shorttitle = {{{XDoG}}},
  author = {Winnem{\"o}ller, Holger and Kyprianidis, Jan Eric and Olsen, Sven C.},
  year = {2012},
  month = oct,
  journal = {Computers \& Graphics},
  volume = {36},
  number = {6},
  pages = {740--753},
  issn = {00978493},
  doi = {10.1016/j.cag.2012.03.004},
  abstract = {Recent extensions to the standard difference-of-Gaussians (DoG) edge detection operator have rendered it less susceptible to noise and increased its aesthetic appeal. Despite these advances, the technical subtleties and stylistic potential of the DoG operator are often overlooked. This paper offers a detailed review of the DoG operator and its extensions, highlighting useful relationships to other image processing techniques. It also presents many new results spanning a variety of styles, including pencil-shading, pastel, hatching, and woodcut. Additionally, we demonstrate a range of subtle artistic effects, such as ghosting, speed-lines, negative edges, indication, and abstraction, all of which are obtained using an extended DoG formulation, or slight modifications thereof. In all cases, the visual quality achieved by the extended DoG operator is comparable to or better than those of systems dedicated to a single style.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\5U2K9UJ6\\WinnemÃ¶ller et al. - 2012 - XDoG An eXtended difference-of-Gaussians compendi.pdf}
}


@misc{fransOutlineColorizationTandem2017,
  title = {Outline {{Colorization}} through {{Tandem Adversarial Networks}}},
  author = {Frans, Kevin},
  year = {2017},
  month = apr,
  number = {arXiv:1704.08834},
  eprint = {1704.08834},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@misc{Undefineda,
  title = {{undefined}},
  journal = {Dwango Media Village(ãƒ‰ãƒ¯ãƒ³ã‚´ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ´ã‚£ãƒ¬ãƒƒã‚¸,dmv)},
  abstract = {undefined},
  howpublished = {https://dmv.nico/en/undefined},
  langid = {ja-jp},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\6TCGT6WP\\nico-opendata.html}
}


@article{branwenDanbooru2021LargeScaleCrowdsourced2015,
  title = {Danbooru2021: {{A Large-Scale Crowdsourced}} and {{Tagged Anime Illustration Dataset}}},
  shorttitle = {Danbooru2021},
  author = {Branwen, Gwern},
  year = {2015},
  month = dec,
  abstract = {Danbooru2021 is a large-scale anime image database with 4.9m+ images annotated with 162m+ tags; it can be useful for machine learning purposes such as image recognition and generation.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/},
  langid = {american},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\FUNQ93W9\\Danbooru2021.html}
}



@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\8A2TWQV3\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}



@misc{shiRealTimeSingleImage2016,
  title = {Real-{{Time Single Image}} and {{Video Super-Resolution Using}} an {{Efficient Sub-Pixel Convolutional Neural Network}}},
  author = {Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P. and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05158},
  eprint = {1609.05158},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@misc{xieAggregatedResidualTransformations2017a,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  year = {2017},
  month = apr,
  number = {arXiv:1611.05431},
  eprint = {1611.05431},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\87MTWYS3\\Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf}
}



@misc{nahDeepMultiscaleConvolutional2018,
  title = {Deep {{Multi-scale Convolutional Neural Network}} for {{Dynamic Scene Deblurring}}},
  author = {Nah, Seungjun and Kim, Tae Hyun and Lee, Kyoung Mu},
  year = {2018},
  month = may,
  number = {arXiv:1612.02177},
  eprint = {1612.02177},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}




@misc{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1784},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\H2Z6M4TQ\\Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf}
}



@misc{ledigPhotoRealisticSingleImage2017,
  title = {Photo-{{Realistic Single Image Super-Resolution Using}} a {{Generative Adversarial Network}}},
  author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  year = {2017},
  month = may,
  number = {arXiv:1609.04802},
  eprint = {1609.04802},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
}



@inproceedings{saitoIllustration2VecSemanticVector2015,
  title = {{{Illustration2Vec}}: A Semantic Vector Representation of Illustrations},
  shorttitle = {{{Illustration2Vec}}},
  booktitle = {{{SIGGRAPH Asia}} 2015 {{Technical Briefs}}},
  author = {Saito, Masaki and Matsui, Yusuke},
  year = {2015},
  month = nov,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{Kobe Japan}},
  doi = {10.1145/2820903.2820907},
  abstract = {Referring to existing illustrations helps novice drawers to realize their ideas. To find such helpful references from a large image collection, we first build a semantic vector representation of illustrations by training convolutional neural networks. As the proposed vector space correctly reflects the semantic meanings of illustrations, users can efficiently search for references with similar attributes. Besides the search with a single query, a semantic morphing algorithm that searches the intermediate illustrations that gradually connect two queries is proposed. Several experiments were conducted to demonstrate the effectiveness of our methods.},
  isbn = {978-1-4503-3930-8},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\3GMYEV2B\\Saito and Matsui - 2015 - Illustration2Vec a semantic vector representation.pdf}
}


@misc{ImageNet,
  title = {{{ImageNet}}},
  howpublished = {https://www.image-net.org/},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\CN22ZTZL\\www.image-net.org.html}
}


@misc{pangImagetoImageTranslationMethods2021,
  title = {Image-to-{{Image Translation}}: {{Methods}} and {{Applications}}},
  shorttitle = {Image-to-{{Image Translation}}},
  author = {Pang, Yingxue and Lin, Jianxin and Qin, Tao and Chen, Zhibo},
  year = {2021},
  month = jul,
  number = {arXiv:2101.08629},
  eprint = {2101.08629},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@misc{wangDiscriminativeRegionProposal2018,
  title = {Discriminative {{Region Proposal Adversarial Networks}} for {{High-Quality Image-to-Image Translation}}},
  author = {Wang, Chao and Zheng, Haiyong and Yu, Zhibin and Zheng, Ziqiang and Gu, Zhaorui and Zheng, Bing},
  year = {2018},
  month = aug,
  number = {arXiv:1711.09554},
  eprint = {1711.09554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.09554},
  abstract = {Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it's still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement "image inpainting" on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\JX69GK9Z\\Wang et al. - 2018 - Discriminative Region Proposal Adversarial Network.pdf}
}


@misc{wangHighResolutionImageSynthesis2018,
  title = {High-{{Resolution Image Synthesis}} and {{Semantic Manipulation}} with {{Conditional GANs}}},
  author = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  year = {2018},
  month = aug,
  number = {arXiv:1711.11585},
  eprint = {1711.11585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}


@misc{tangMultiChannelAttentionSelection2019,
  title = {Multi-{{Channel Attention Selection GAN}} with {{Cascaded Semantic Guidance}} for {{Cross-View Image Translation}}},
  author = {Tang, Hao and Xu, Dan and Sebe, Nicu and Wang, Yanzhi and Corso, Jason J. and Yan, Yan},
  year = {2019},
  month = apr,
  number = {arXiv:1904.06807},
  eprint = {1904.06807},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.06807},
  abstract = {Cross-view image translation is challenging because it involves images with drastically different views and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top datasets show that our model is able to generate significantly better results than the state-of-the-art methods. The source code, data and trained models are available at https://github.com/Ha0Tang/SelectionGAN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia}
}


@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\HMSH3QZG\\Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf}
}



@misc{baoBEiTBERTPreTraining2021,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  shorttitle = {{{BEiT}}},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu},
  year = {2021},
  month = jun,
  number = {arXiv:2106.08254},
  eprint = {2106.08254},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.08254},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\RSHP8GLK\\Bao et al. - 2021 - BEiT BERT Pre-Training of Image Transformers.pdf}
}




@misc{weiMaskedFeaturePrediction2021,
  title = {Masked {{Feature Prediction}} for {{Self-Supervised Visual Pre-Training}}},
  author = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  year = {2021},
  month = dec,
  number = {arXiv:2112.09133},
  eprint = {2112.09133},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09133},
  abstract = {We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7\% with MViT-L on Kinetics-400, 88.3\% on Kinetics-600, 80.4\% on Kinetics-700, 38.8 mAP on AVA, and 75.0\% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}



@misc{newellHowUsefulSelfSupervised2020,
  title = {How {{Useful}} Is {{Self-Supervised Pretraining}} for {{Visual Tasks}}?},
  author = {Newell, Alejandro and Deng, Jia},
  year = {2020},
  month = mar,
  number = {arXiv:2003.14323},
  eprint = {2003.14323},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \textbackslash href\{https://www.github.com/princeton-vl/selfstudy\}\{github.com/princeton-vl/selfstudy\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\etsun\\Zotero\\storage\\BDFV5PGV\\Newell and Deng - 2020 - How Useful is Self-Supervised Pretraining for Visu.pdf}
}


@misc{sahariaPaletteImagetoImageDiffusion2022,
  title = {Palette: {{Image-to-Image Diffusion Models}}},
  shorttitle = {Palette},
  author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2111.05826},
  eprint = {2111.05826},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.05826},
  abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}



@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05233},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}




@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}







