\chapter{Introductory}
\label{chapterlabel1}
\section{Background}

\subsection{2D Animation Pipeline}

The process of creating 2D animation consists of some labor intensive tasks, improving the efficiency of them could have a huge effect on the industry as a whole. By saving time and money, we will have greater opportunities to improve the quality and quantity of work done. 
The existing workflow is segmented into following stages:

\begin{enumerate}
    \item \textbf{Key Poses / Layout} Setting up major actions and placement.
    \item \textbf{Roughs / Inbetweening} Assign timing and frame distribution.
    \item \textbf{Cleanup / Inking} Clean sketches, add styling and motion effects.
    \item \textbf{Coloring / Mattes} Coloring, add shadows and highlights.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/introduction/stages.png}
    \caption{Sample frame for 4 stages of the workflow, including Layout, Roughs, Cleanup and Color.} 
    \label{fig:stages}
\end{figure}

Currently, lead artist gives front-loaded creative input followed by labor intensive tasks that he has little control. We want lead artist to get an accurate preview of what might be the final product. In particular, when they create the basis for character movement. We plan to create tools that enhance and speed up the process of creating 2D animation. The 2D animator will be offered more creative options in both movement and styles.

\subsubsection{Proposed System}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/introduction/proposed_workflow.png}
    \caption{Overview of proposed final workflow for the artist. The artist will produce a number of rough frames, with timing and style information. They will be feed into multiple machine learning (ML) pipelines to generate a preview of the final animation.} 
    \label{fig:proposed_worflow}
\end{figure}

In the proposed system, the artist will produce a number of rough keyframes with timing and styles information. These frames will go through 4 ML pipelines, each with different purposes: (1) clean up the rough sketches; (2) coloring; (3) frame interpolation (increase fps) and (4) upscaling (super resolution). Each method will be researched and a prototype model will be developed for proof-of-concept thesis (which is this thesis).


\section{General Approach}
\subsection{Image-to-Image Translation}
As you may have observed, all four tasks falls into the field of Image-to-Image translation (I2I), which is a sub-field in Computer Vision. I2I refers to the task of transforming images from one domain to another, so that they have the styles or characteristics from another domain. It has been gaining popularity in recent year due to its wide range of applications in computer vision problems such as image restoration, super resolution, segmentation and pose estimation. The scope of this project falls into two-domain, supervised I2I\cite{pangImagetoImageTranslationMethods2021}. Most recent research on I2I uses deep convolutional neural network to learn a mapping function between and source and target domain. Pix2pix\cite{isolaImagetoImageTranslationConditional2018} first successfully apply deep convolutional conditional GAN to solve a wide range of I2I problems. However, Pix2pix itself suffers from a number of issues. For example, it often produce blurry result due to use of L2 loss which minimizes the average loss\cite{wangDiscriminativeRegionProposal2018}; training might be unstable and error-prone as resolution increases\cite{wangHighResolutionImageSynthesis2018}; and unable to capture complex scene structure\cite{tangMultiChannelAttentionSelection2019}. Thus we will not directly apply it in our tasks, however, most models are either inspired by or follow the same architecture as Pix2pix.

\subsection{Pretraining}
Many researches found that in computer vision tasks, it is generally useful to first pretrain model with large amount of data and then finetune to downstream tasks\cite{baoBEiTBERTPreTraining2021, weiMaskedFeaturePrediction2021, newellHowUsefulSelfSupervised2020}.

Pretraining can be seem as a form of weight initialization, and weight initialization is important when training neural network. In the early days, some researchers initialize their neural network's weight to all zeros, and they soon finds that this results in the \textit{symmetry problem} (see Appendix \ref{app:ml:sym}) which makes the network harder to learn. Some researchers proposed random initialization, which works well in some cases, but if you are unlucky, you can run into initialization that is either too large or too small, it will results in exploding and vanishing gradient problem respectively. An popular alternative is Xavier initialization\cite{glorotUnderstandingDifficultyTraining2010}. Its idea is to initialize weights such that mean of the activations are zeros, and the variance of the activations stay the same across every layer. In other words, weights of a layer $l$ are randomly sampled from a Gaussian/normal distribution with mean $\mu = 0$ and variance $\sigma^2=\frac{1}{n_{l-1}}$, where $n_{l-1}$ is the number of neurons in the previous layer, and bias are initialized with zeros.

While Xavier can improve training process mathematically, the network will likely starts far from the optimum. Pretraining on the other hand, when applied on a similar task, allow you to start closer to the optimum. It can significantly reduce training time and reuse pattern learnt. Additionally, it can also reduces the chance of vanishing/exploding gradient. Beside finetuning for downstream tasks, it can also used as a feature extractor to enhance network's capability, or used as a loss function that determine the feature loss

\todo[inline]{TODO: introduce pretraining researches}

%Pretraining + finetuning + feature content loss + WGAN-GP, cGAN + U-Net

\subsection{Diffusion Model}
Recently, diffusion model has been gaining popularity and shown comparative benchmarks on various computer vision tasks\cite{sahariaPaletteImagetoImageDiffusion2022, dhariwalDiffusionModelsBeat2021}. Although it is relatively new and has not been studies extensively and I did not apply them directly. I will briefly introduce them in case of interest in the future because it is highly potential.

Diffusion model can be traced back to 2015, where researchers proposed a generative machine learning approach that is highly flexible and computationally tractable\cite{sohl-dicksteinDeepUnsupervisedLearning2015}. The idea is to develop a forward diffusion process that systematically and slowly erase the structure in the data, and then learn a backward diffusion process that recovers it. However, it did not attract much attention. Later, a stochastic differential equation that transform complex data distribution to a known prior distribution based on the diffusion process was presented\cite{songScoreBasedGenerativeModeling2021}. It showed that diffusion model can produce comparative performance in certain image generation tasks like inpainting by slowly introducing Gaussian noise into the forward diffusion process, and then learn its reverse process. A denoising diffusion model\cite{hoDenoisingDiffusionProbabilistic2020} also showed that it is capable of generating high quality images.

\todo[inline]{TODO: introduce diffusion model researches}


\section{Evaluation Methods}
%mostly by eye, we can compute the PSNR and SSIM, but, generally eye is enough.
%Some stuff about things.\cite{example-citation} Some more things. 

%Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.
%\blindtext
