\chapter{Preliminaries}
\label{chapterlabel2}

\section{U-Net}
U-Net\cite{ronnebergerUNetConvolutionalNetworks2015} is commonly refers as a U-shaped encoder-decoder residual network architecture. It is originally developed for image segmentation in biomedical image processing (see figure \ref{fig:unet_arch}) based on a previous fully convolutional network study\cite{longFullyConvolutionalNetworks2015}. It is a popular approach in many vision tasks.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/preliminary/unet_arch.png}
    \caption{U-Net architecture. Light blue box corresponds to multi-channels features; blue arrow is 3x3 convolution with ReLU; gray arrow is copy and crop operations, more commonly known as skip/residual connection; red arrow is 2x2 max pooling; green arrow is 2x2 up convolution or interpolation; and light blue arrow is 1x1 convolution for feature reduction.\cite{ronnebergerUNetConvolutionalNetworks2015}} 
    \label{fig:unet_arch}
\end{figure}

The first half of the network consists of a number of encoder block. Each consists of a convolutional block followed by a ReLU activation and max pooling. Each block halves the size of feature map. The second half is similar to the first half, except it doubles the size of feature map at each decode block. The output of the corresponding encoder block is given as additional input to the decoder network, this operation is known as skip/residual connection.

Some implementation adds a batch normalization between the convolution layer and the ReLU layer, in attempts to reduces internal covariance shift and improve training stability. Dropout layer may also added after ReLU as a form of regularization to improve generalization of the network.

\section{GAN}

GAN stands for Generative Adversaial Network\cite{goodfellowGenerativeAdversarialNetworks2014a}, it is a machine learning framework where two networks simultaneously compete and learns from each other in the form of two-players, zero sum game. The network consists of a generator $\mathcal{G}$ tries to mimic the target and a discriminator $\mathcal{D}$ tries to distinguish between real and generated input. This training framework is commonly used in computer vision tasks to generate high quality images.

Major challenges exists in training GAN, it can fail easily due to various reasons such as mode-collapse (see appendix \ref{app:ml:mode_col}), vanishing gradient (see appendix \ref{app:ml:van_grad}), non-convergence (see appendix \ref{app:ml:non_conv}), instability, inappropriate design (e.g. under/overpowered discriminator, use of loss and optimization algorithm, etc.). Therefore a number of variations are proposed to improve it, and they are commonly adopted in most model describe in this paper.


\subsection{Wasserstein GAN}
Wasserstein GAN\cite{arjovskyWassersteinGAN2017} is a type of GAN proposed to improve the stability of training vanilla GAN. Compared to vanilla GAN, Wasserstein GAN provides a better gradient, which is helpful for getting rid of problems such as mode collapse, and provides more meaningful training curves for debugging and hyper-parameter tuning.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/preliminary/wgan_vs_gan_grad.png}
    \caption{Comparing the optimal Wasserstein function (scaled) and optimal GAN discriminator (scaled) given a fixed generator distribution and reference distribution. We can see that the gradient of Wasserstein function is equals to $1$ for the optimal case, and gives less variance compared to optimal GAN discriminator.\cite{FileWassersteinGANCritic}} 
    \label{fig:wgan_vs_gan_grad}
\end{figure}

Wasserstein GAN uses Wasserstein distance to compute the loss between actual and target distribution. It has the advantage of producing more consistent values when comparing distributions compared to other functions such as KL-divergence and JS-divergence. This means that its gradients have smaller variance and thus training is more stable (see figure \ref{fig:wgan_vs_gan_grad}). It is also practically useful because it is highly efficient to compute by satisfying the \textit{dual representation theorem} in a metric space:

$$
W_{1}(\mu, \nu)=\frac{1}{K} \sup _{\|f\|_{L} \leq K} \mathbb{E}_{x \sim \mu}[f(x)]-\mathbb{E}_{y \sim \nu}[f(y)]
$$

where $W_1$ is the Wasserstein distance; $K$ is any fixed $K > 0$; $\sup$ means supremum (the largest, in other words, the upper bound); and $||\cdot||_L$ is the Lipschitz norm (Lipschitz constant with boundary condition)\footnote{A proof can be found at \url{https://en.wikipedia.org/wiki/Wasserstein\_metric\#Dual\_representation\_of\_W1}}. There are several methods for enforcing this constraint during training. The two most common methods are weight clipping (see appendix \ref{app:ml:weight_clip}) and gradient penalty (see appendix \ref{app:ml:grad_pen}).






\subsection{WGAN-GP}

\subsection{cGAN}

\section{ResNet}

\subsection{ResNeXt}

\section{Pix2pix}

\section{Pixel Shuffle}
